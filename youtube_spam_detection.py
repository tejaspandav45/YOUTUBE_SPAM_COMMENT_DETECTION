# -*- coding: utf-8 -*-
"""Youtube_spam_detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZbbphPYTgUMoiJTtyJlB2p1uEdU-_VcW

#**YOUTUBE SPAM COMMENT DETECTION**
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from IPython.display import display
import ipywidgets as widgets

# List of dataset URLs
dataset_urls = [
    "/content/Youtube01-Psy.csv",
    "/content/Youtube02-KatyPerry.csv",
    "/content/Youtube03-LMFAO.csv",
    "/content/Youtube04-Eminem.csv",
    "/content/youtube05-Shakira.csv"
]

# Load and combine datasets
dfs = [pd.read_csv(url) for url in dataset_urls]
combined_df = pd.concat(dfs, ignore_index=True)

# Split the combined dataset
train_data, test_data, train_labels, test_labels = train_test_split(
    combined_df['CONTENT'], combined_df['CLASS'], test_size=0.2, random_state=42
)

# Tokenize the comments
max_words = 10000  # Adjust as needed
tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')
tokenizer.fit_on_texts(train_data)

# Convert comments to sequences
train_sequences = tokenizer.texts_to_sequences(train_data)
test_sequences = tokenizer.texts_to_sequences(test_data)

# Pad sequences for uniform length
max_length = 100  # Adjust as needed
train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post', truncating='post')
test_padded = pad_sequences(test_sequences, maxlen=max_length, padding='post', truncating='post')

# Build the model
embedding_dim = 16  # Adjust as needed
model = Sequential([
    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_length),
    LSTM(128, activation='sigmoid', return_sequences=True),  # ReLU activation added
    Dropout(0.5),  # Dropout for regularization
    LSTM(64, activation='sigmoid'),
    Dense(32, activation='sigmoid'),  # Additional dense layer with ReLU activation
    Dense(1, activation='sigmoid')
])

model.compile(optimizer='ADAM', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(train_padded, train_labels, epochs=40, validation_data=(test_padded, test_labels))

# Evaluate the model
predictions = model.predict(test_padded)
binary_predictions = [1 if pred > 0.5 else 0 for pred in predictions]

# Print evaluation metrics
accuracy = accuracy_score(test_labels, binary_predictions)
print(f'Accuracy: {accuracy}')
print('Classification Report:')
print(classification_report(test_labels, binary_predictions))

# Define a function for real-time spam detection
def predict_spam(comment):
    sequence = tokenizer.texts_to_sequences([comment])
    padded_sequence = pad_sequences(sequence, maxlen=max_length, padding='post', truncating='post')
    prediction = model.predict(padded_sequence)[0][0]
    return "Spam" if prediction > 0.5 else "Not Spam"

def real_time_detection(comment):
    prediction = predict_spam(comment)
    print(f"Comment: {comment}\nPrediction: {prediction}")

# Create a text input widget
comment_input = widgets.Textarea(description='Enter comment:', value='')

# Define a callback function
def on_submit(b):
    real_time_detection(comment_input.value)

# Create a button to submit the comment
submit_button = widgets.Button(description='Submit')
submit_button.on_click(on_submit)

# Display widgets
display(comment_input)
display(submit_button)